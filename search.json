[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Manateam",
    "section": "",
    "text": "To explain why, you’ll meet our three members along with our herd leader, Dr. Conrad. We’ll talk about how we got here, what we’re doing, and how we think this work is important.\n\n\n\nMaddie Hare\n\nMaddie’s academic history is just that; she graduated from Dalhousie University in 2017 and 2021 with her BA and MA in History, respectively. She is currently in her second year of the Master of Information program at Dalhousie. She hopes to continue in academia and direct her efforts at bridging disciplinary divides with digital tools!\n\nJuan Chaves Baquero\n\n[insert bio here]\n\nPoppy Riddle\n\nFrom the San Francisco Bay Area in California, Poppy along with her family moved to Halifax in 2020. Having graduated from Dalhousie in 2022 with a Masters in Information Management, she loved writing lit reviews so much she decided to work on a Phd spanning information science and human-computer interaction.\n\nDr. Colin Conrad\n\nDr. Conrad, (Assistant Professor of information technology in the Faculty of Management at Dalhousie University) is an interdisciplinary information technology scholar whose work draws from human-computer interaction, neuroscience, education, and data science. He is interested in how attention or other brain mechanisms impact the effectiveness of learning technologies. Recent work uses a variety of research methods borrowed from neuroscience (such as EEG and eye tracking) and information science (such as surveys and data mining) to answer research questions.\n\n\n\n\n\nThis year, we are developed tools to introduce humanities researchers to digital humanities by exploring how text-based analysis might help them develop insights. We are using machine learning to explore text found in archives from scanned documents turning them into concise topics and word exploration. We are also introducing them to two technologies that might be gateways to further exploration: the Python coding language and Tableau, a data visualization and analysis tool.\n\n\n\nDigital Humanities seeks to provide a means of synthesis for huge amounts of information, enabling researchers to explore phenomena on a scale that is impractical for manual methods. It does not seek to replace human interpretation but enable interpretation of information that is hidden due to its size, length of time, or access. Python, a widely used coding language, is great for data science and can provide a means of not only exploration but functional programs to be used by researchers. Tableau, available as both free and commercially licensed versions, provides an easy path without coding to explore massive amounts of data. We think both of these tools are a gateway for researchers to exploring electronic resources freely available or of their own creation."
  },
  {
    "objectID": "chapter-1.html",
    "href": "chapter-1.html",
    "title": "Getting started",
    "section": "",
    "text": "You will need a Google account to use the notebooks, which are on Google Colab. You can set on up Set up new Google account.\nThis does not have to be your personal account, you can set one up just for data analysis, if you like.\nIf you already have one, feel free to skip ahead to Chapter 2."
  },
  {
    "objectID": "chapter-3.html",
    "href": "chapter-3.html",
    "title": "The OCR notebook",
    "section": "",
    "text": "You can find the OCR notebook here on GitHub.\n\n\nAfter using the OCR notebook you will have accomplished the following:\n\n\n\nGoogle Colab is a Jupyter Notebook that runs on Google’s servers, allowing you to run Python code on data calculated and stored on cloud servers. This is great for large, computationally heavy projects where processing can be increased easily with a few lines of code. Unlike Jupyter notebooks run locally on your computer, the Colab environment is independent of your locally installed environments, meaning that you are less likely to run into conflicts. The downside to the Google Colab environment is that any information created in Colab is erased when you close the browser, unless you save it somewhere, like GitHub or locally on your computer.\n\n\n\n\n\nThere are two ways to open the OCR notebook:\n\nopen through Github [insert screenshot] - you will use this process the first time opening the notebooks.\n\nSave a copy of this notebook to your Google Drive. All changes will be saved to your own copy.\n\nOpen through Colab [insert screenshot] - You can use this process if you have previously saved it to your Google Drive.\n\nIn Colab, go to File>Open. This will open a dialog box.\nAt the top tabs of the dialog box, select Google Drive - you should see your previously saved copy.\n\n\nRun the first cell - this will connect to your Google Drive. A dialog box will open and ask you to select your Google account and ask for your permission to connect to your Google Drive.\nIt will also create a new directory in your Google Drive for all of the files needed for this project. You should see something like this indicating your new working directory.\n\nFollow the steps for 1d to put the scanned pdf files into the working directory. Colab has a file manager on the left hand side that will allow you to drag and drop files to the working directory, OCR_Project_Folder.\n\n\n\n\nInstall the dependencies and import the libraries. IF you have run this before, you may find that the runtime needs to be restarted. Its ok to do this. Just click the Restart Runtime button provided. Rerun the cell again to made sure everything has been imported.\n\n\n\nThis step gathers any and all PDF files you have placed in the working directory. What you are first starting, it is recommended that you only do one at a time, if you are working with multipage PDFs, such as those from the Gazette archives.\n\nIF you are running this analysis again, make sure you have cleaned out previous PDFs and only have the ones you want in the working directory.\nThe second cell creates the OsCaRizer function. This does all of the analysis on the PDF.\n\n\n\n\nRun this cell to take each image file and create text in a .txt file format. The output file will have the same name as the input file, but with a .txt extension.\n\n\n\n\nUnlike your own computer, Google Colab does not save anything in memory. However, since you’ve saved everything to your Google Drive, such as your results, these files will be saved indefinitely.\nThe PDF file and the output .txt files should be moved out of the working directory to a safe place, such as another Google Drive Directory. You will likely be using these in the next notebook for LDA.\nThe image files may remain as it is safe for them to be overwritten.\n\n\n\nEven for those scholars that prefer close reading, the OCR process can make reading primary texts easier as it converts scanned images into searchable text.\nAs a raw text file, there are improvements that can be made to this notebook. Improvements could be:\n\nkeep articles continuous as they are often split across pages in newspapers.\nidentify image placements with placeholder text.\ncombine with image classification methods that identify and provide context for image and text together.\nidentify other aspects of the artefact, such as damage locations or typefaces."
  },
  {
    "objectID": "chapter-2.html",
    "href": "chapter-2.html",
    "title": "Source material",
    "section": "",
    "text": "PDF files work best for this analysis, but there are three types of PDF files to be aware of:\n\ndigitally-created pdfs such as from print streams in Word, that contain text and font-family information, and are searchable.\nscanned/image-only pdfs that contain no text information\nsearchable pdfs - where there is a text layer that has been developed with OCR and sits underneath the scanned image\n\nArchives will typically use scanned image-only pdfs, though sometimes these are saved as PDF/A, which is a reduced file size with reduced functionality. For our Gazette project, you can find high quality scans of documents here at the Dalhousie Gazette Archives. You can find the current Gazette here.\n\nThe following is an example screenshot of a multi-page scanned image PDF file of a 1922 Dalhousie Gazette.\n\n\n\nYou can also use other source materials as you get comfortable with this process. Some examples include:\n\nimages of texts taken with your smartphone.\n\n\n\n\nThe OCR notebook we provide uses PyTesseract, which is a Python wrapper for Google’s Tesseract OCR library. It is not intended for analyzing handwritting. This article on medium.com illustrates some of the challenges. For analysis of handwriting, you will want Human Handwritten Text Recognition (HTR). For those with Python experience and time to create training datasets, you can find more here."
  },
  {
    "objectID": "chapter-4.html",
    "href": "chapter-4.html",
    "title": "The LDA notebook",
    "section": "",
    "text": "You can find the LDA notebook here.\n\n\nsome text about what Colab is and why we’re using it.\n\n\n\nsome text here going through the steps.\n\n\n\nMore text here on managing the data\n\n\n\nthis is where I’ll talk about future improvements and how the notebook might be useful in other ways."
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "license",
    "section": "",
    "text": "CC BY: This license allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. The license allows for commercial use.\n\n\n\n\ncc by logo\n\n\nCC BY includes the following elements:\nBY – Credit must be given to the creator"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manateam Documentation on OCR and LDA",
    "section": "",
    "text": "Welcome to our documentation introducing you to Python-based digital humanities tools."
  },
  {
    "objectID": "index.html#what-is-digital-humanities",
    "href": "index.html#what-is-digital-humanities",
    "title": "Manateam Documentation on OCR and LDA",
    "section": "What is Digital Humanities?",
    "text": "What is Digital Humanities?\nDigital Humanities (DH) is the application of digital tools to process information to enable researchers to explore subjects in the humanities in new ways. Digital tools can provide advantages to the researcher with processing capabilities on huge amounts of information on texts, images, sound, or even on data itself. While DH employs digital technology with the goal is deriving new insights, these same tools can also be the subject of critical inquiry as well. It is a huge field with simultaneous developments in other fields. The Wikipedia entry on DH is well worth reading for an introduction, as are the resources from University of Victoria, Stanford, and the University of California Berkeley.\nOur workshop will focus on text analysis in which we look for topics, though there are many types of text analysis."
  },
  {
    "objectID": "index.html#this-documentation-covers-the-following",
    "href": "index.html#this-documentation-covers-the-following",
    "title": "Manateam Documentation on OCR and LDA",
    "section": "This documentation covers the following:",
    "text": "This documentation covers the following:\n\nChapter 1 - Getting started\nChapter 2 - Finding source material\nChapter 3 - Using the OCR notebook\nChapter 4 - Using the LDA notebook\nChapter 5 - Moving on and using your own data"
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "Manateam Documentation on OCR and LDA",
    "section": "Objectives",
    "text": "Objectives\nOur workshop seeks to provide the following for our participants: \n\nUse Python to run an Optical Character Recognition (OCR) library called PyTesseract to extract text from a scanned PDF file and perform topic modeling using LDA to derive meaningful topics.\nCreate visualizations, with both Python and Tableau Public, to generate meaningful topics from the text.\nDescribe basic elements of Python, Google Colab and Tableau Public.\nCover the basics of Jupyter Notebooks hosted in the Google Colab environment.\nExplore the basic principles and limitations of topic modeling.\nDiscuss open source software and publicly available information can be implemented in social sciences and humanities research.\nUse Tableau Public to explore and analyze data using no coding, but able to produce visualizations and tables.\nCover the process to create topics from a sample covering 100 years of the Dalhousie Gazette."
  },
  {
    "objectID": "index.html#what-is-ocr",
    "href": "index.html#what-is-ocr",
    "title": "Manateam Documentation on OCR and LDA",
    "section": "What is OCR?",
    "text": "What is OCR?\n\nOCR is Optical Character Recognition is a means of converting scanned or an image of text into searchable text that can be used for analysis. OCR has wide applications in text-to-speech, text mining and natural language processing, translation, and computer vision.\nOCR has a long history as far back as the early 1900’s, when text was optically converted into tones as an assistive technology. Today, mobile applications come with every smartphone enabling translation of signs across many languages. Google Books and Project Gutenberg are other examples of scanned image to text OCR processing.\nOur version of OCR is a Python wrapper for Google’s Tesseract which has advantages on unusual fonts or poor quality scans.\nThe input for our Python library, Pytesseract, uses PDF files. There are three types of PDF files:\n\ndigitally-created pdfs such as from print streams in Word, that contain text and font-family information, and are searchable.\nscanned/image-only pdfs that contain no text information\nsearchable pdfs - where there is a text layer that has been developed with OCR and sits underneath the scanned image\n\nAs there can be challenges with pdfs, Pytesseract will first convert all of them to images regardless of their type, process the characters using neural networks, and output the files as a plain text stream. We will then use the text files for further analysis using LDA topic modeling."
  },
  {
    "objectID": "index.html#what-is-lda",
    "href": "index.html#what-is-lda",
    "title": "Manateam Documentation on OCR and LDA",
    "section": "What is LDA?",
    "text": "What is LDA?\n\nLDA, Latent Dirichlet Allocation, is a probability-based natural language processing (NLP) algorithm, that is specifically used to identify unknown topics within a text. While not the current state of the art, it is robust and still works pretty well!\nLDA is one of a growing number of topic modelling methods used for information retrieval, enabling linked data and semantic web searching. For digital humanities researchers, we can identify topics across groups of documents, though it still relies upon the expertise of the observer to interpret ambiguous terms.\n\nReady? Move on to Chapter 2, Getting Started."
  }
]