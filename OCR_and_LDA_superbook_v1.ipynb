{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DigitalHugManitees/DH_Topic_Workshop/blob/main/OCR_and_LDA_superbook_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSNtr-fvyJAk"
      },
      "source": [
        "# OCR and LDA for pdf files.\n",
        "This super notebook is for a concise analysis of the dalhousie gazette pdf files. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "1. sign into your google account\n",
        "2. save a copy of this notebook to your google drive as your own\n",
        "3. run cell linking to your google drive\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#mount google drive here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os \n",
        "\n",
        "# Set your working directory to a folder in your Google Drive. \n",
        "# the base Google Drive directory\n",
        "root_dir = \"/content/drive/My Drive/\"\n",
        "# choose where you want your project files to be saved\n",
        "project_folder = \"Colab Notebooks/OCR_Project_Folder/\"\n",
        "\n",
        "def create_and_set_working_directory(project_folder):\n",
        "  # check if your project folder exists. if not, it will be created.\n",
        "  if os.path.isdir(root_dir + project_folder) == False:\n",
        "    os.mkdir(root_dir + project_folder)\n",
        "    print(root_dir + project_folder + ' did not exist but was created.')\n",
        "\n",
        "  # change the OS to use your project folder as the working directory\n",
        "  os.chdir(root_dir + project_folder)\n",
        "\n",
        "  # show me the current working directory\n",
        "\n",
        "  print('\\nYour working directory was changed to ' + root_dir + project_folder)\n",
        "\n",
        "create_and_set_working_directory(project_folder)\n",
        "\n",
        "print()\n",
        "print('Move your pdf files to the working directory: ' + project_folder)"
      ],
      "metadata": {
        "id": "094KlAekz_dZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8109d4-70fd-4ee0-cd45-10a0aff29c57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Your working directory was changed to /content/drive/My Drive/Colab Notebooks/OCR_Project_Folder/\n",
            "\n",
            "Move your pdf files to the working directory: Colab Notebooks/OCR_Project_Folder/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Install dependencies and libraries\n",
        "\"\"\"\n",
        "\n",
        "!apt install tesseract-ocr\n",
        "!apt install libtesseract-dev\n",
        "!apt-get install poppler-utils\n",
        "!pip install pdf2image\n",
        "!pip install Pillow\n",
        "!pip install pytesseract\n",
        "\n",
        "# Import libraries\n",
        "import pytesseract\n",
        "from PIL import ImageEnhance, ImageFilter, Image\n",
        "import sys\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "\n",
        "#sometimes poppler install fails - this line should update everything\n",
        "!apt-get update "
      ],
      "metadata": {
        "id": "2kn5EFby87qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66e7f0a-cf87-48a9-ff75-a6798fd54ce1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.00~git2288-10f4998a-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 26 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.00~git2288-10f4998a-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 26 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (0.62.0-2ubuntu2.14).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 26 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.8/dist-packages (1.16.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from pdf2image) (9.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (9.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.8/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/dist-packages (from pytesseract) (21.3)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=21.3->pytesseract) (3.0.9)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Optional: Change runtime type from CPU to GPU.\n",
        "\"\"\"\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNfxf6SgOv0S",
        "outputId": "e3c62fda-9e02-4ead-d541-ec8d475ae0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 13926517785464551111\n",
              " xla_global_id: -1]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 3: gather the PDF files and create the OsCaRizer function\n",
        "\n",
        "This should find all PDF files in working dir. \n",
        "HOWEVER, it will not find those nested in subdirectories\n",
        "\"\"\"\n",
        "path_of_the_directory = os.getcwd()\n",
        "ext = ('.pdf')\n",
        "pdf_list = []\n",
        "for files in os.listdir(path_of_the_directory):\n",
        "    if files.endswith(ext):\n",
        "        print(files)\n",
        "        pdf_list.append(files)\n",
        "\n",
        "    else:\n",
        "        continue\n",
        "print('done - move on to next cell')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1rm_EaXgDxT",
        "outputId": "c4466553-4725-4fc1-80bf-0dbe18bf598c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dalhousiegazette_volume54_issue13_may_1922__1_.pdf\n",
            "done - move on to next cell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mJBI1h74yJAp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "create function to run in a loop\n",
        "through the pdf list\n",
        "\n",
        "\"\"\"\n",
        "def OsCaRizer(pdf_file):\n",
        "\n",
        "  # Path of the pdf\n",
        "  #PDF_file = '/content/drive/MyDrive/Colab Notebooks/OCR_Project_Folder/dalhousiegazette_volume54_issue19_november_29_1922.pdf'\n",
        "  PDF_file = path_of_the_directory + \"/\" + pdf_file\n",
        "\n",
        "  #suppress warnings about image size from PIL\n",
        "  Image.MAX_IMAGE_PIXELS = None   # disables the warning\n",
        "    \n",
        "  # Store all the pages of the PDF in a variable\n",
        "  pages = convert_from_path(PDF_file, 500)\n",
        "    \n",
        "  # Counter to store images of each page of PDF to image\n",
        "  image_counter = 1\n",
        "    \n",
        "  # Iterate through all the pages stored above\n",
        "  for page in pages:\n",
        "    \n",
        "      # Declaring filename for each page of PDF as JPG\n",
        "      # For each page, filename will be:\n",
        "      # PDF page 1 -> page_1.jpg\n",
        "      # PDF page 2 -> page_2.jpg\n",
        "      # PDF page 3 -> page_3.jpg\n",
        "      # ....\n",
        "      # PDF page n -> page_n.jpg\n",
        "      filename = \"page_\"+str(image_counter)+\".jpg\"\n",
        "        \n",
        "      # Save the image of the page in system\n",
        "      page.save(filename, 'JPEG')\n",
        "    \n",
        "      # Increment the counter to update filename\n",
        "      image_counter = image_counter + 1\n",
        "  \"\"\"\n",
        "  process the image files to text\n",
        "  \"\"\"\n",
        "  #suppress warnings about image size from PIL\n",
        "  Image.MAX_IMAGE_PIXELS = None   # disables the warning\n",
        "\n",
        "  # Variable to get count of total number of pages\n",
        "  filelimit = image_counter-1\n",
        "    \n",
        "  # Creating a text file to write the output\n",
        "  outfile = ((pdf_file.rsplit( \".\", 1)[0]) + '.txt')\n",
        "    \n",
        "  # Open the file in append mode so that \n",
        "  # All contents of all images are added to the same file\n",
        "  f = open(outfile, \"a\")\n",
        "    \n",
        "  # Iterate from 1 to total number of pages\n",
        "  for i in (range(1, filelimit + 1)):\n",
        "    \n",
        "      # Set filename to recognize text from\n",
        "      # These files will be:\n",
        "      # page_1.jpg\n",
        "      # page_2.jpg\n",
        "      # ....\n",
        "      # page_n.jpg\n",
        "      filename = \"page_\"+str(i)+\".jpg\"\n",
        "            \n",
        "      # Recognize the text as string in image using pytesserct\n",
        "      text = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
        "    \n",
        "      # The recognized text is stored in variable text\n",
        "      # Any string processing may be applied on text\n",
        "      # Here, basic formatting has been done:\n",
        "      # In many PDFs, at line ending, if a word can't\n",
        "      # be written fully, a 'hyphen' is added.\n",
        "      # The rest of the word is written in the next line.\n",
        "      # To remove this, we replace every '-\\n' to ''.\n",
        "      text = text.replace('-\\n', '')    \n",
        "    \n",
        "      # Finally, write the processed text to the file.\n",
        "      f.write(text)\n",
        "    \n",
        "  # Close the file after writing all the text.\n",
        "  f.close()\n",
        "  \"\"\"\n",
        "  clean out page image files before next instance of loop\n",
        "  \"\"\"\n",
        "\n",
        "  files = os.listdir(path_of_the_directory)\n",
        "  for f in files:\n",
        "    if not os.path.isdir(f) and \"page\" in f:\n",
        "      os.remove(f)\n",
        "\n",
        "  print('done - you should see a .txt file named after your .pdf file in your working directory')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DmDWsIwByJAq",
        "outputId": "eba3ff40-7538-4702-a070-dbdc2a30b482",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 4: Runs the OsCaRizer function.\n",
        "\"\"\"\n",
        "\n",
        "for k in tqdm(range(len(pdf_list))):\n",
        "    pdf_file = pdf_list[k]\n",
        "    OsCaRizer(pdf_file)\n",
        "    print('the file ' + pdf_file + ' is complete')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA using Spacey and Gensim \n"
      ],
      "metadata": {
        "id": "CySBr4U2B6JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#**************************************\n",
        "# Install libraries and dependencies\n",
        "#*************************************\n",
        "\n",
        "# Install libraries and dependencies\n",
        "!pip install pyLDAvis -qq \n",
        "!pip install -qq -U gensim\n",
        "!pip install spacy -qq\n",
        "!pip install matplotlib -qq\n",
        "!pip install seaborn -qq\n",
        "!python -m spacy download en_core_web_md -qq\n",
        "!pip install fsspec\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # this ignores warnings\n",
        "# Import\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import spacy\n",
        "import pyLDAvis.gensim_models\n",
        "pyLDAvis.enable_notebook()# Visualise inside a notebook\n",
        "import en_core_web_md\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models import CoherenceModel\n",
        "import itertools\n",
        "import os\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import itertools\n",
        "\n",
        "pd.set_option('display.max_columns', None) # this allows you see all columns in pandas\n",
        "\n",
        "# Our spaCy model:\n",
        "nlp = en_core_web_md.load() # this will be used to train the algorithm"
      ],
      "metadata": {
        "id": "xDTqNKeKCZ9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Import your data\n",
        "\n",
        "This should find all .txt files in working dir. \n",
        "HOWEVER, it will not find those nested in subdirectories\n",
        "\"\"\"\n",
        "\n",
        "path_of_the_directory = os.getcwd()\n",
        "ext = ('.txt')\n",
        "\n",
        "#define empty df\n",
        "reports = pd.DataFrame(columns=['file_name','publication_year','content'])\n",
        "txt_list = []\n",
        "txt_file_name = []\n",
        "year_list = []\n",
        "content_list = []\n",
        "#get all txt files in the Drive \n",
        "for files in os.listdir(path_of_the_directory):\n",
        "\n",
        "  if files.endswith(ext):\n",
        "      print(files)\n",
        "      txt_list.append(files)\n",
        "\n",
        "      # Creating a text file to write the output\n",
        "      file_name = ((files.rsplit( \".\", 1)[0]))\n",
        "      txt_file_name.append(file_name)\n",
        "\n",
        "      #get year\n",
        "      year_value = ((file_name.rsplit( \"_\")[-1]))\n",
        "      year_list.append(year_value)\n",
        "\n",
        "      #read files into list\n",
        "      temp_read = Path(files).read_text()\n",
        "      content_list.append(temp_read)\n",
        "\n",
        "  else:\n",
        "      continue\n",
        "#use to gather lists in a temp df\n",
        "temp_df = pd.DataFrame.from_records(itertools.zip_longest(txt_file_name, year_list, content_list), columns=['file_name','publication_year','content'])\n",
        "\n",
        "# take temp_df and concat with perm_df\n",
        "reports = pd.concat([reports, temp_df], ignore_index=True)\n",
        "\n",
        "# change this to match your content column name. \n",
        "reports['content'] = reports['content'].astype(str)\n",
        "print()\n",
        "print('This shows the data types')\n",
        "print(reports.dtypes)\n",
        "print()\n",
        "print('The following is our collected data')\n",
        "reports.head()"
      ],
      "metadata": {
        "id": "AskG3MjtCgyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "set number of n-grams here\n",
        "\"\"\"\n",
        "\n",
        "from nltk import ngrams\n",
        "\n",
        "def compile_ngrams(text, number_of_n=3, include_unigrams=True):\n",
        "  ngram_list = []\n",
        "  # number_of_n controls up to how many n we build an ngram for\n",
        "  # 2 being bigrams, 3 being trigrams, ect.\n",
        "  for n in range(number_of_n):\n",
        "    if n == 0 and not include_unigrams:\n",
        "      continue\n",
        "    for ngram in ngrams(text.split(), n + 1):\n",
        "      ngram_list.append(' '.join(ngram))\n",
        "  return ngram_list\n",
        "\n",
        "\"\"\"this is a test to make sure the function works\"\"\"\n",
        "print(compile_ngrams(\"There was a cloud computing conference about big data and natural language processing\"))"
      ],
      "metadata": {
        "id": "tKTpawIwC2GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "data cleaning, setting controlled vocabulary and tokenizing\n",
        "\"\"\"\n",
        "\n",
        "# Tags I want to remove from the text\n",
        "removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
        "tokens = []\n",
        "\n",
        "# Words I really care about that should certainly be in the dictionary\n",
        "author_assigned_keywords = [\"dalhousie university\", \"dalhousie gazette\"] #place anything in here in quotes\n",
        "\n",
        "for summary in nlp.pipe(reports['content']):\n",
        "   # build up tokens here\n",
        "   # using the authors heuristics:\n",
        "   unigrams = [token.lemma_.lower() for token in summary if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
        "   # using ngrams\n",
        "   # we can use the proj_tok clean text as the \"paragraph\"\n",
        "   proj_tok = compile_ngrams(\" \".join(unigrams), number_of_n=2, include_unigrams=False)\n",
        "   # using \"author assigned keywords\" by checking if they are in the text\n",
        "   proj_tok += [keyword for keyword in author_assigned_keywords if keyword in summary.text]\n",
        "   tokens.append(proj_tok)\n",
        "\n",
        "\n",
        "# Add tokens to new column\n",
        "reports['tokens'] = tokens \n",
        "\n",
        "# Create dictionary\n",
        "# Apply the Dictionary Object from Gensim, which maps each word to their unique ID:\n",
        "dictionary = Dictionary(reports['tokens'])\n",
        "print(dictionary.token2id)\n",
        "\n",
        "#see the dataframe with a new column called tokens\n",
        "reports"
      ],
      "metadata": {
        "id": "Uttdc8YFC8ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "set number of topics\n",
        "\"\"\"\n",
        "\n",
        "# Filter dictionary\n",
        "\"\"\"\n",
        "the following dictionary filter can result in no terms. adjust to lower values for smaller documents. \n",
        "see: https://stackoverflow.com/questions/40840731/valueerror-cannot-compute-lda-over-an-empty-collection-no-terms\n",
        "\"\"\"\n",
        "#dictionary.filter_extremes(no_below=5, no_above=0.2, keep_n=1000)\n",
        "\n",
        "# Create corpus\n",
        "corpus = [dictionary.doc2bow(doc) for doc in reports['tokens']]\n",
        "\n",
        "# Optimal LDA model building\n",
        "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=100, num_topics=22, workers = 4, passes=100)\n",
        "\n",
        "# Print topics\n",
        "lda_model.print_topics(-1)\n",
        "\n",
        "# Where does a text belong to\n",
        "lda_model[corpus][0]\n",
        "reports['content'][0]\n",
        "\n",
        "# Visualize topics\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(lda_display)\n",
        "\n",
        "# Save the report\n",
        "pyLDAvis.save_html(lda_display, 'index.html')"
      ],
      "metadata": {
        "id": "QJqiG_poDJm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Print topics\n",
        "\"\"\"\n",
        "# Where does a text belong to\n",
        "temp_clusters = []\n",
        "for i in range(len(lda_model[corpus])):\n",
        "  print(lda_model[corpus][i][0])\n",
        "  temp_clusters.append(lda_model[corpus][i][0])\n",
        "\n",
        "df2 = pd.DataFrame(temp_clusters, columns=['cluster', 'score'])\n",
        "\n",
        "#let's see those clusters and the some tokens\n",
        "#for i in range(0, lda_model.num_topics-1):\n",
        "for i in range(lda_model.num_topics):\n",
        "  print (\"cluster: \" + str(i+1) + \"  \" + str( lda_model.print_topic(i)))\n",
        "\n",
        "#create an ordered list and join to reports\n",
        "reports = pd.concat([reports,df2], axis=1)\n",
        "\n",
        "#drop the content column as you already have this in the originall l.txt file\n",
        "reports = reports.drop(columns='content')\n",
        "\n",
        "#reposition cluster and score\n",
        "reports = reports[['file_name', 'publication_year', 'cluster', 'score', 'tokens']]\n",
        "\n",
        "# correct cluster number as ordinal\n",
        "reports['cluster'] = reports['cluster']+1\n",
        "\n",
        "#export as excel file\n",
        "reports.to_csv(\"LDA_with_clusters.csv\", index=False) \n",
        "\n",
        "#let's see it!\n",
        "reports\n"
      ],
      "metadata": {
        "id": "y2oDTLKRDOXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "coherence tests\n",
        "\"\"\"\n",
        "# Coherence score using C_umass:\n",
        "topics = []\n",
        "score = []\n",
        "for i in range(1,20,1):\n",
        "    lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)\n",
        "    cm = CoherenceModel(model=lda_model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
        "    topics.append(i)\n",
        "    score.append(cm.get_coherence())\n",
        "_=plt.plot(topics, score)\n",
        "_=plt.xlabel('Number of Topics')\n",
        "_=plt.ylabel('Coherence Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i4f1adUGDY8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}